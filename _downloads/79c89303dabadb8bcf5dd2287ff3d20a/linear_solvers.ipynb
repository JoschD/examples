{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n\n# Linear Equation Solvers\n\nA quick summary for linear equation solvers available in python packages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dependencies:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport cvxpy as cp\nimport scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple System of Linear Equations\n\nLet's say we want to solve an equation system\n\n\\begin{align}A \\cdot x = y\\end{align}\n\n\n### Inverse \nIf the equation system is exact solvable, i.e. if the inverse of A exists,\nwe can easily calculate \n\n\\begin{align}x = A^{-1} \\cdot y\\end{align}\n\nfrom numpy's linalg functions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 2], \n              [4, 6]])\ny = np.array([3, 6])\n\n# Solve.\nx = np.linalg.inv(A).dot(y)\n\n# Print result.\nprint(f\"{x = !s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>| **Don't!**\n      | Don't ever do that! \n\n      Inverting a matrix is numerically much more unstable than \n      using proper solvers for the whole equation system.</p></div>\n\n### Solve\nIt is better to let numpy solve this for you:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 2], \n              [4, 6]])\ny = np.array([3, 6])\n\n# Solve.\nx = np.linalg.solve(A, y)\n\n# Print result.\nprint(f\"{x = !s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But, this only works on quadratic matrices.\nIt fails if the matrix does not have an inverse:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 2, 5], \n              [4, 6, 1]])\ny = np.array([3, 6])\n\n# 1) Solve via inverse.\ntry:\n    x = np.linalg.inv(A).dot(y)\nexcept np.linalg.LinAlgError as e:\n    print(f\"1) {e.__class__.__name__}: {e!s}\")\n\n# 3) Solve with `solve` function.\ntry:\n    x = np.linalg.solve(A, y)\nexcept np.linalg.LinAlgError as e:\n    print(f\"2) {e.__class__.__name__}: {e!s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Least Squares\nYet, we can still find a solution for x\nvia the linear least squares algorithm implemented in numpy.\nIt finds the x that minimizes the norm of the residual: \n\n\\begin{align}\\min_{x} \\left\\lVert A \\cdot x - y \\right\\rVert\\end{align}\n\nIn *underdetermined* systems it finds an *exact* solution:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 2, 5], \n              [4, 6, 1]])\ny = np.array([3, 6])\n\n# Solve.\nx, residuals, rank, sv = np.linalg.lstsq(A, y, rcond=None)\n\n# Print result.\nprint(f\"{x = !s}\")\nprint(f\"{A @ x = !s}\")\nprint(f\"{residuals = !s}\")\nprint(f\"{rank = !s}  (of A)\")\nprint(f\"{sv = !s} (singular values of A)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the same function optimizes the residuals for\nan *overdetermined* equation system. \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 1], \n              [6, 1], \n              [4, 6]])\ny = np.array([3, 6, 8])\n\n# Solve.\nx, residuals, rank, sv = np.linalg.lstsq(A, y, rcond=None)\n\n# Print result.\nprint(f\"{x = !s}\")\nprint(f\"{A @ x = !s}\")\nprint(f\"{residuals  = !s}\")\nprint(f\"{rank = !s}  (of A)\")\nprint(f\"{sv = !s} (singular values of A)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Equations with constraints\n\n\n\\begin{align}A \\cdot x = y\\end{align}\n\nsuch that\n\n\\begin{align}B \\cdot x \\leq z\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you need an exact solution for your equalities,\none can abuse `scipy`'s `linprog`, which finds \na solution to the problem\n\n\\begin{align}\\min_{x} c^T x\\end{align}\n\nsuch that\n\n\\begin{align}A_{ub} \\cdot x \\leq b_{ub} \\\\\n   A_{eq} \\cdot x = b_{eq} \\\\\n     l \\leq x \\leq u\\end{align}\n\nWhere we can set $c$ to zero, as we don't care about the minimization.\nUsing the linear least-squares example above, we had one value > 0.6.\nLet us force all values of x to be below 0.6. \nWe only need to plug-in our equality system (eq) and the bounds:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 2, 5], \n              [4, 6, 1]])\ny = np.array([3, 6])\nc = np.zeros(A.shape[1])\n\n# Solve.\nresult = scipy.optimize.linprog(c=c, A_ub=None, b_ub=None, A_eq=A, b_eq=y, bounds=[[None, 0.6]]*3)\nx = result.x\n\n# Print result.\nprint(result.message)\nprint(f\"{x = !s}\")\nprint(f\"{A @ x = !s}\")\nprint(f\"{result.con = !s} (i.e. residuals)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, or if our bounds come from another equation system, \nwe can give upper-bounds system (ub) instead of the `bounds`:\n(In this case, this leads to better residuals!)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nA = np.array([[1, 2, 5], \n              [4, 6, 1]])\ny = np.array([3, 6])\nB = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nz = 0.6 * np.ones(B.shape[1])\nc = np.zeros(A.shape[1])\n\n# Solve.\nresult = scipy.optimize.linprog(c=c, A_ub=B, b_ub=z, A_eq=A, b_eq=y, bounds=None)\nx = result.x\n\n# Print result.\nprint(result.message)\nprint(f\"{x = !s}\")\nprint(f\"{A @ x = !s}\")\nprint(f\"{result.slack = !s} (i.e. z - x)\")\nprint(f\"{result.con = !s} (i.e. residuals)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, this does **not** minimize \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data.\nm = 20\nn = 15\nnp.random.seed(1)\nA = np.random.randn(m, n)\nb = np.random.randn(m)\n\n# Define and solve the CVXPY problem.\nx = cp.Variable(n)\ncost = cp.sum_squares(A @ x - b)\nprob = cp.Problem(cp.Minimize(cost))\nprob.solve()\n\n# Print result.\nprint(\"\\nThe optimal value is\", prob.value)\nprint(\"The optimal x is\")\nprint(x.value)\nprint(\"The norm of the residual is \", cp.norm(A @ x - b, p=2).value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nonlinear Optimization\n\nIf the problem is non-linear or with error-bars, there are a lot of functions available\n\n- | `scipy.optimize.least_squares(fun, x0, jac='2-point', bounds=(- inf, inf)) <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares>`_ \n  | Solve a nonlinear least-squares problem with bounds on the variables.\n\n- | `scipy.optimize.curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(- inf, inf)) <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html#scipy.optimize.curve_fit>`_ \n  | Use non-linear least squares to fit a function, f, to data. *(Basically a wrapper around least_squares)*\n\n- | `scipy.optimize.linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='interior-point', callback=None, options=None, x0=None) <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html>`_ \n  | Linear programming: minimize a linear objective function subject to linear equality and inequality constraints.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thumbnail for the Sphinx-Gallery:\nsphinx_gallery_thumbnail_path = '_static/thumb_linear_solvers.png'\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}